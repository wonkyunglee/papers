\section{Train Phase}
이번 섹션에서는 vector representation E 를 학습시키기 위한 방법들을 설명한다. E를 우리의 의도대로 임베딩시키기 위해 제안되었던 방법들과 각 방법 별 문제점, 그리고 해결하기 위한 방법들을 나열하였다. 그리고 우리가 설계한 최종 목적함수와 constraint 를 소개한다. 


\subsection{Single label classification}
 딥러닝을 통해 IR 시스템에서 사용될 vector representation 을 학습시키는 방법은 여러가지가 있지만[ref ] 우리는 Auxiliary task 인 Classification task 를 학습시키면서 얻은 vector representation 을 활용한다. Classification task 를 학습시키기 위한 목적함수는 다음과 같다.

% Numbered Equation
\begin{equation}
\label{eqn:01}
\min_{\theta,w,b} J(\theta, w, b) = -\sum_i{y_{si} \log{\hat{y_i}}}
\end{equation}

where 
\[
\hat{y_i} = \frac{exp(w*h(v_i;\theta)+b)}{\sum_i{exp(w*h(v_i;\theta)+b)}}
\]

 이를 통해 멀웨어 샘플들의 vector representation 은 선형 classifier 로 구분가능한 공간에 위치하게 되고, 랭킹 모듈로 리트리벌할 수 있게 된다. 이 모델의 장점은 벡터 스페이스 모델을 사용함으로써 얻는 이점들이다. 특히 continuous similarity 를 사용한 랭킹이 가능해진다는 점이 이전 모델들에 비해 발전이라고 할 수 있다. 하지만 한계들도 많이 존재한다. 같은 레이블이 붙어있다고 해서 정말 의미적으로 가깝거나 다른 레이블이 붙어있다고 해서 정말 의미적으로 다른 샘플이라고 보장할 수 없는 멀웨어 레이블링의 모호성 때문에 싱글레이블만으로 임베딩을 하면 임베딩 공간과 우리의 멀웨어에 대한 인지 공간이 매우 상이하게 된다. 


\subsection{Multi label learning}
single label learning 의 문제를 해결하기 위해 우리는 멀웨어에 멀티레이블링을 한 데이터셋을 멀티레이블 분류 태스크로 학습시켜, 샘플들 간 위치를 좀 더 의미적으로 유사하게 임베딩한다. 이렇게 했을 때, 같은 대분류 내의 소분류 클래스들에 해당하는 샘플들이 가까운 곳에 임베딩되게 된다. objective function 은 다음과 같다. 

\begin{equation}
\label{eqn:02}
\min_{\theta,w,b} J(\theta, w, b) = -\sum_i{ \sum_j{ y_{mij} \log{\hat{y_{ij}}}}}
\end{equation}
where  
\[
\hat{y_{ij}} = \frac{exp(w*h(v_i;\theta)+b)}{ 1 + exp(w*h(v_i;\theta)+b)}
\]

하지만 이렇게 임베딩한 샘플들도 inner class variance 가 너무 커서 ranking module 에서 같은 클래스 내 두 샘플의 거리가 서로 다른 클래스 내의 두 샘플 간 거리보다 더 커지는 현상이 발생하게 된다. 


\subsection{Mutli label centerloss classification}
multilabel learning 에서의 문제를 해결하기 위해 우리는 메트릭 러닝을 위한 목적함수를 추가하여 최종 목적함수를 설계하였다. 센터로스[ref]를 변형한 목적함수를 통해 innerclass variance 를 낮추고 intraclass variance 를 높였다. 최적화할 식은 다음과 같다. 

\begin{equation}
\label{eqn:03}
\min_{\theta,w,b} J(\theta, w, b) = -\sum_i{\sum_j{ y_{mij} \log{\hat{y_{ij}}}}}
\end{equation}

s.t.

\begin{equation}
\label{eqn:04}
 h(v_i;\theta)) - \sum_k{c_{ik}} < \epsilon , i = \{1,2,3, ..., N\}
\end{equation}

위의 constraint optimization 의 Lagrangian 은 다음 식으로 정의된다.

\begin{equation}
\label{eqn:05}
L(\theta, w, b, \lambda) = -\sum_i{[ \sum_j{ y_{mij} \log{\hat{y_{ij}}}} + \lambda * (\sum_k{c_{ik}} - h(v_i;\theta))^2]} 
\end{equation}
where 
\[
\hat{y_{ij}} = \frac{exp(w*h(v_i;\theta)+b)}{ 1 + exp(w*h(v_i;\theta)+b)},
\]

$\lambda$ is KKT multiplier and $c_{ik}$ is a template vector of kth label of ith sample.

위의 식은 최종 로스 형태가 시그모이드 크로스엔트로피와 센터로스를 1:람다의 비율로 더한 것과 같다는 것을 의미한다. 
\[
\text{total loss} = \text{sigmoid crossentropy loss} + \lambda*\text{center loss}
\]



싱글레이블 센터로스를 제안했던 논문에서는 각 레이블 별로 템플릿 벡터를 메모리에 저장해놓고 그 템플릿 벡터와 보틀넥의 거리가 작아지도록 MSE 에러를 로스에 추가했다. 우리도 마찬가지로 각 레이블별로 템플릿 벡터를 메모리에 저장해놓고, 보틀넥이 정답 레이블 벡터들의 합이 되도록 MSE 에러를 로스에 추가했다. 그리고, 보틀넥과 정답 레이블벡터의 합의 차이에 alpha/the number of answer labels 만큼의 곱을 하여 각 레이블을 업데이트해준다. 알파는 하이퍼파라미터로, 템플릿 벡터와 보틀넥을 얼마나 섞어서 기존 템플릿벡터를 업데이트할지를 결정해주는 값이다. 하지만 섹션 9의 결과에서 볼 수 있듯이, 아직 인간의 인지와 시멘틱 임베딩 공간이 일치하지는 않는다. 그 이유는 각 태그 별 중요도를 반영하여 임베딩하지 않았기 때문이다. 


\subsection{Mutli label weighted centerloss classification}
agent 나 downloader같은 태그들에 대한 innerclass variance 를 낮추는 것보다는 오히려 수는 적지만 중요한 태그들인 ransom, coinminer 등의 태그들의 innerclass variance 를 줄이는 것이 Ranking 할 때 더 의미가 있다. 따라서 태그들의 중요도를 constraint 로 추가한 최종 목적함수를 사용한다. 최적화 할 문제는 다음과 같다.  

\begin{equation}
\label{eqn:06}
\min_{\theta, w, b} J(\theta, w, b) = -\sum_i{ \sum_j{ y_{mij} \log{\hat{y_{ij}}}}}
\end{equation}

s.t.

\[
h(v_i;\theta)) - \sum_k{c_{ik}} < \epsilon ,
\]


\[
||c_i||_2 = cc_i
\]

for

\[
 i = \{1,2,3, ..., N\}
\]

위의 constrained optimization problem 의 Lagrangian 은 다음과 같다.

\begin{equation}
\label{eqn:07}
\begin{split}
L(\theta, w, b, \lambda_1, \lambda_2) = -\sum_i{[ \sum_j{ y_{mij} \log{\hat{y_{ij}}}}} \\  
+ \lambda_1 * (\sum_k{c_{ik}} - h(v_i;\theta))^2 + \lambda_2 * (||c_i||_2 - cc_i)^2 ]
\end{split}
\end{equation}

where 
\[
\hat{y_{ij}} = \frac{exp(w*h(v_i;\theta)+b)}{ 1 + exp(w*h(v_i;\theta)+b)}
\]

실제로 학습할때 메모리에 새로운 레이블 템플릿 벡터를 다음의 방법으로 업데이트하여 두 번째 constraint를 강제로 만족시켰다. 
\begin{equation}
\label{eqn:08}
c_i \leftarrow c_i + (1-\alpha) * (e_i - \sum_k{c_{ik}}) \\  
\end{equation}



