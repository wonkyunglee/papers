\section{Auxiliary Task}
이번 섹션에서는 vector representation E 를 학습시키기 위한 방법들을 설명한다. E를 우리의 의도대로 임베딩시키기 위해 제안되었던 방법들과 각 방법 별 문제점, 그리고 해결하기 위한 방법들을 나열하였다. 그리고 우리가 설계한 최종 목적함수와 constraint 를 소개한다. 


\subsection{Single label classification}
 딥러닝을 통해 IR 시스템에서 사용될 vector representation 을 학습시키는 방법은 여러가지가 있지만[ref ] 우리는 Auxiliary task 인 Classification task 를 학습시키면서 얻은 vector representation 을 활용한다. Classification task 를 학습시키기 위한 목적함수는 다음과 같다. 이를 통해 멀웨어 샘플들의 vector representation 은 선형 classifier 로 구분가능한 공간에 위치하게 되고, 랭킹 모듈로 리트리벌할 수 있게 된다. 이 모델의 장점은 벡터 스페이스 모델을 사용함으로써 얻는 이점들이다. 특히 continuous similarity 를 사용한 랭킹이 가능해진다는 점이 이전 모델들에 비해 발전이라고 할 수 있다. 하지만 한계들도 많이 존재한다. 멀웨어는 같은 레이블이 붙어있다고 해서 정말 가깝거나 다른 레이블이 붙어있다고 해서 정말 다른 샘플이라고 보장할 수 없는 레이블 특성 때문에 싱글레이블만으로 임베딩을 하면 시멘틱 공간과 우리의 인지 공간이 매우 상이하게 된다. 

% objective function = sum_{y log_y_hat}, where y_hat = softmax( h(V) )


\subsection{Multi label learning}
single label learning 의 문제를 해결하기 위해 우리는 멀웨어에 멀티레이블링을 한 데이터셋을 멀티레이블 분류 태스크로 학습시켜, 샘플들 간 위치를 좀 더 의미적으로 유사하게 임베딩한다. 이렇게 했을 때, 같은 대분류 내의 소분류 클래스들에 해당하는 샘플들이 가까운 곳에 임베딩되게 된다. objective function 은 다음과 같다. 하지만 이렇게 임베딩한 샘플들도 inner class variance 가 너무 커서 ranking module 에서 같은 클래스 내 두 샘플의 거리가 서로 다른 클래스 내의 두 샘플 간 거리보다 더 커지는 현상이 발생하게 된다. 



\subsection{Mutli label centerloss classification}
multilabel learning 에서의 문제를 해결하기 위해 우리는 메트릭 러닝을 위한 목적함수를 추가하여 최종 목적함수를 설계하였다. 센터로스[ref]를 변형한 목적함수를 통해 innerclass variance 를 낮추고 intraclass variance 를 높였다. 목적함수의 최종 식은 다음과 같다. 하지만 아직 인간의 인지와 시멘틱 임베딩 공간이 일치하지는 않는다. 

% objective function = sigmoid + centerloss


\subsection{Mutli label weighted centerloss classification}
agent 나 downloader 같은 태그들에 대해서 같은 샘플들을 모아봤자 의미가 없다. 오히려 수는 적지만 중요한 태그들인 ransom, coinminer 등의 태그들이 Ranking 할 때 좀 더 중요한 역할을 해주었으면 한다. 따라서 태그들의 중요도를 constraint 로 추가한 최종 목적함수를 사용한다. 목적함수는 다음과 같다.  

% objective function = sigmoid + centerloss where ||centers|| = c_i
