\section{Evaluation}
In this section, we evaluate the performance of our MR system in several ways, and verify that we have performed the semantic-aware malware retrieval task well.

\begin{table*}[!htb]%apk_result
\caption{APK19000 Querying Results}
\label{tab:apk_result}
\begin{minipage}{\textwidth}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Dataset             & \multicolumn{6}{c|}{Querying trainset}                      & \multicolumn{6}{c|}{Querying validset}                      \\ \hline
Metric              & \multicolumn{3}{c|}{Precision}  & \multicolumn{3}{c|}{Weighted precision} & \multicolumn{3}{c|}{Precision}  & \multicolumn{3}{c|}{Weighted precision} \\ \hline
k              & top1   & top10  & top100 & top1      & top10     & top100   & top1   & top10  & top100 & top1      & top10     & top100   \\ \hline
Weighted centerloss (sum)  & 0.9846 & 0.9743 & 0.9374 & 0.9869    & \textbf{0.9802}   & \textbf{0.955}    & 0.8717 & 0.862  & 0.8272 & 0.8808    & 0.8746    & 0.8534   \\ \hline
Weighted centerloss (avg) & \textbf{0.9856} & 0.9772 & \textbf{0.9481} & 0.9855    & 0.9779    & 0.9528   & \textbf{0.8929 }& \textbf{0.8837 }& 0.8559 & \textbf{0.8893   } & 0.8815    & 0.8591   \\ \hline
Centerloss (sum)         & 0.9843 & 0.9737 & 0.9358 & 0.984     & 0.9733    & 0.9388   & 0.8736 & 0.8676 & 0.8373 & 0.8752    & 0.872     & 0.8462   \\ \hline
Centerloss (avg)        & 0.987  & \textbf{0.9776} & 0.9474 & 0.9872    & \textbf{0.9776}    & 0.9472   & 0.8845 & 0.8779 & \textbf{0.8562} & 0.8892    & \textbf{0.8829}    & \textbf{0.8631}   \\ \hline
Multi-label (baseline2)               & 0.9764 & 0.9617 & 0.9115 & 0.9752    & 0.9593    & 0.9094   & 0.8871 & 0.8713 & 0.8184 & 0.8869    & 0.8704    & 0.8236   \\ \hline
Single-label (baseline1)              & 0.9035 & 0.8727 & 0.8061 & 0.9004    & 0.8667    & 0.7946   & 0.8489 & 0.8231 & 0.7446 & 0.8434    & 0.8151    & 0.733    \\ \hline
\end{tabular}
\end{center}
\bigskip\centering
\end{minipage}
\end{table*}%


\begin{table*}[!htb]%pe_result
\caption{PE1300 Querying Results}
\label{tab:pe_result}
\begin{minipage}{\textwidth}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Dataset             & \multicolumn{6}{c|}{Queyring trainset}                      & \multicolumn{6}{c|}{Queyring validset}                      \\ \hline
Metric              & \multicolumn{3}{c|}{Precision}  & \multicolumn{3}{c|}{Weighted precision} & \multicolumn{3}{c|}{Precision}  & \multicolumn{3}{c|}{Weighted precision} \\ \hline
k              & top1   & top10  & top100 & top1      & top10     & top100   & top1   & top10  & top100 & top1      & top10     & top100   \\ \hline
Weighted centerloss (sum)  & 1      & 1      & 0.9487 & 1         & 1         & 0.9329   & 0.9082 & 0.9075 & 0.8875 & 0.8816    & 0.881     & 0.8547   \\ \hline
Weighted centerloss (avg) & 1      & 1      & \textbf{0.9613} & 1         & 1         & \textbf{0.9671}   & \textbf{0.9203} & \textbf{0.9186} & \textbf{0.9029 }& \textbf{0.9142}    &\textbf{ 0.9085}    & \textbf{0.8916}   \\ \hline
Centerloss (sum)         & 1      & 1      & 0.9483 & 1         & 1         & 0.9319   & 0.9058 & 0.9058 & 0.8839 & 0.8845    & 0.8845    & 0.8487   \\ \hline
Centerloss (avg)        & 1      & 1      & 0.9544 & 1         & 1         & 0.965    & 0.9179 & 0.9179 & 0.8999 & 0.9048    & 0.9048    & 0.888    \\ \hline
Multi-label (baseline2)               & 0.9877 & 0.9665 & 0.7949 & 0.9877    & 0.9668    & 0.7793   & 0.8913 & 0.8894 & 0.8033 & 0.8666    & 0.8676    & 0.7692   \\ \hline
Single-label (baseline1)              & 0.9647 & 0.9004 & 0.7397 & 0.9608    & 0.8961    & 0.7235   & 0.8865 & 0.8597 & 0.7805 & 0.8784    & 0.8413    & 0.7501   \\ \hline
\end{tabular}
\end{center}
\bigskip\centering

\end{minipage}
\end{table*}%


\subsection{Implementation and Setup}
\textbf{Datasets. } 
The dataset used for learning and evaluation of the model is composed of samples crawling from VirusTotal \cite{total2012virustotal} and certified by security expert. In addition, Labeling was automated from the detection results of dozens of AntiVirus engines provided by VirusTotal. The detection results of each AntiVirus engine in VirusTotal are parsed in a meaningful word tokens, and the security expert selected labels from the word tokens. In addition, labels were prioritized according to their importance. This saves the time of the security expert in labeling the malware samples, while achieving accurate labels. Finally, for one sample, we have two types of labels: a representative label and a multi-label. The multi-label includes a representative label. Among the datasets, train set and test set were randomly sampled by 7: 3. And the distribution of samples over the representative label is even.

\begin{itemize}
	\item{ \textbf{Dataset 1. PE1300. } Among the more than 52,351 PE samples crawled from VirusTotal, the security expert verified about 1,300 PE samples with 11 labels. Four of the labels are representative labels and the number of samples for the representative label is almost the same.
	}
	\item{ \textbf{Dataset 2. APK19000. } In the case of APK, there are 19,000 samples from 192,342 samples crawled by VirusTotal, which were verified by expert, and used 83 kinds of labels. Of these, 19 labels are representative labels, and the number of samples for each representative label is 1,000.
	}
\end{itemize}

$ $

\textbf{Settings. }

The feature extracter module extracts the malware image feature \cite{nataraj2011malware} and resize it to 224 by 224.
The network structure of the neural embedder module is convolutional neural netowrk which has five convolutional layers and two fully connected layers using batch normalization \cite{ioffe2015batch}. The previous layer of the last layer used leaky relu \cite{xu2015empirical} as activation function and the rest of the activation function are relu \cite{nair2010rectified}. 
The malware image \cite{nataraj2011malware} is passed through neural embedder. The convolutional layers extract features to contain local shift invariant characteristics of the malignant region or the region that characterizes the malware.

The hyper parameters used in the experiment are as follows. Adam optimizer \cite{kingma2014adam} was used as the optimizer and the learning rate was 0.001. $\alpha$ in centerloss is 0.1. When learning the PE1300 dataset, we used 0.1 as $\lambda$, which is the ratio of cross-entropy loss and centerloss, and 0.001, when learning APK19000. 


\textbf{Models. }
There are six models used in the experiment. First, the single-label model is a first baseline of our experiments, in which the auxiliary task classifies representative label using softmax-cross-entropy as a loss function. The multi-label model is a deep learning model designed to classify multiple labels into multi-task learning using a sigmoid cross-entropy as a loss function. This is a second baseline of our experiment. Next, to find out the effect of applying centerloss, we added centerloss to the multi-label model. There are two models that combine semantic components as average and summation. And to see the effect of scoring, here is the weighted center loss model with the importance coefficient added as a constraint. This method also has two versions of average and summation models.
  

\begin{table*}[!htb]%class_variances
\caption{Class Variances}
\label{tab:class_variances}
\begin{minipage}{\textwidth}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Dataset             & \multicolumn{3}{c|}{APK19000}                               & \multicolumn{3}{c|}{PE1300}                                             \\ \hline
Variance              & Intra-class  & Inter-class  & Inter/Intra & Intra-class  & Inter-class  & Inter/Intra  \\ \hline
Weighted centerloss (sum)  & 0.0473              &         2.793       &     59.0486             & 0.129               &              2.169 &              16.8140    \\ \hline
Weighted centerloss (avg) & \textbf{0.0101}              &          \textbf{    1.1906} &    \textbf{117.8812 }            & \textbf{0.0361             } &                   \textbf{ 1.2698} &      \textbf{35.1745}                   \\ \hline
Centerloss (sum)         & 0.2268              &                    5.2967 &          23.3541               & 0.1748              &              1.8299       &    10.4685                     \\ \hline
Centerloss (mean)        & 0.0258              &                    1.2785 &           49.5543              & 0.1332              &        1.6826             &     12.6321                    \\ \hline
Multi-label (baseline2)               & 2.4956              &                    21.8435 &       8.7528                    & 25833.582           &        840.54             &        0.0325               \\ \hline
Single-label (baseline1)              & 1.2103              &                    15.18 &            12.5423             & 2310.7649           &      267.683               &           0.1158              \\ \hline
\end{tabular}
\end{center}
\bigskip\centering
\end{minipage}
\end{table*}%


\begin{table}[!htb]%auxiliary_result 
\caption{Auxiliary Task Results}
\label{tab:auxiliary_result}
\begin{minipage}{\columnwidth}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Dataset             & APK19000 & PE1300 \\ \hline
Weighted center (sum)  & 0.9618   & 0.9801 \\ \hline
Weighted center (avg) & 0.9636   & \textbf{0.9812} \\ \hline
Centerloss (sum)         & \textbf{0.9728 }  & 0.9753 \\ \hline
Centerloss (avg)        & 0.9705   & 0.9574 \\ \hline
Multi-label(baseline2)          & 0.9606   & 0.9495 \\ \hline
Single-label(baseline1)              & 0.9508   & 0.946  \\ \hline
\end{tabular}
\end{center}
\bigskip\centering
\end{minipage}
\end{table}%


\subsection{Querying Quantitative Test}

In this part, we will analyze the effect of centerloss by comparing precision at top K value obtained by querying with malware samples for each model and class variance of representation vectors.

\textbf{Metrics. } The metrics used in quantitative evaluation are as follows. We used precision to evaluate the performance of MR system, weighted precision to evaluate scoring, and intra and inter class variance.

\begin{itemize}
	\item{ \textbf{Precision} 
	
	Let the labels of the querying sample $q$ be $y_{q_1}, y_{q_2}, ..., y_{q_m}$. Let the labels of the $k$ search result samples $r_1, r_2, ..., r_k$ obtained from Ranking module $R$ be $y_{r_1}, y_{r_2}, ..., y_{r_l}$ respectively . In this case, precision at top K is obtained as follows.

	\[
	Precision_{K} = \frac{1}{N} *\sum_{i=1}^{N}{ \frac{ \sum_{k=1}^{K}{|Y_{q_i} \cap Y_{r_k}|}}{  \sum_{k=1}^{K}{ |Y_{r_k}| }  }}
	\]\\
	where $|Y|$ is the number of elements of set $Y$.
	}
	\item{ \textbf{Weighted precision} 
	
	Weighted precision is calculated in the same way as precision, adding weighted importance coefficients of each label. This metric provides better results when model hits important labels.

	\[
	WeightedPrecision_{K} = \frac{1}{N} *\sum_{i=1}^{N}{ \frac{ \sum_{k=1}^{K}{|Y_{q_i} \cap Y_{r_k}|_w}}{  \sum_{k=1}^{K}{ |Y_{r_k}|_w }  }}
	\]\\
	where $|Y|_w = \sum_i{c_{y_i}}$ and $c_j$ is importance coefficient of j'th label.
	}
	\item{ \textbf{Inner class variance} 
	
	Inner class variance represents the variance of the representation vector of samples in the same class. That is, it is a metric that measures how much the same classes are gathered together. Because our samples have a multi-label, we measure the variance for a sample with the same label combination and average it.
  
	}
	\item{ \textbf{Inter class variance} 
	
	Inter class variance is a metric that indicates how far apart other classes are. Likewise, the distances between sample vectors with different label combinations were measured and average it.
 
	}
	%\item{ \textbf{inter class variance over inner class variance} 
	%inter cliass variance 대비 inner class variance 를 측정하여 여러 모델의 임베딩 벡터 셋들이 얼마나 IR 이 잘 비교할 수 있게 된다.}
\end{itemize}



\textbf{Sample querying results. }
From the results of Table. \ref{tab:apk_result} and Table. \ref{tab:pe_result}, we were able to observe the following. First, the precision is better than the multi-label baseline model when using centerloss. Especially, as the K increases, precision value of the multi-label baseline model falls sharply, but the centerloss models do not. Second, the models with the importance coefficient added to the centerloss have better weighted precision results than the other models. Finally, we observed that using average as a combination of semantic components was slightly better than using summation method.

From the above experiments, we can confirm that the above quantitative values ​​are generally higher when the MR system is constructed using multi-label centerloss. This shows that the semantic understanding attribute of desired property of the MR system is more satisfactory than the other methods. We also showed that scoring is possible in MR systems using weighted centerloss with deep learning.

\textbf{Class variances. }
We measured the class variance to determine the cause of the above results. The results can be seen in the Table. \ref{tab:class_variances}. From this result, we confirmed that using centerloss reduces the inner-class variance. In addition, to measure the relative class variance of the models, the intra-class variance and the inter-class variance of the embedding vectors were measured and compared. The larger the value of the inter-class variance / intra-class variance is, the more discriminative the representation vectors are, meaning that retrieval results may be better.

\textbf{Auxiliary task results. }
Table.\ref{tab:auxiliary_result} represents the AUC for the validation set of the auxiliary task.The purpose of the original Auxiliary task is literally a subtask to learn the representation vector to be used in the MR system, but with the centerloss, we can see a slight performance improvement.


\subsection{Querying Qualitative Test}
In this part, we list the top k results obtained by querying the MR system we built in the three ways introduced in Section 4.1. Thus, our proposal shows that the performance of the MR system has improved. For each querying test, we can see the querying method in the Figure. \ref{fig:qualitative_all}.

\textbf{Querying by malware sample. }
A sample of the APK malware with a semantic component consisting of \texttt{agent}, \texttt{slocker}, \texttt{jisut}, and \texttt{ransom} was queried to the learned MR system with the models described above. The retrieval result can be observed in Table.\ref{tab:sample_query_result}. The system learned by the centerloss (add) model is better than the other two models.
\[
   query = \mathbf{e_q} 
\]


% Table
\begin{table*}%sample_query_result
\caption{Queyring by sample}
\label{tab:sample_query_result}
\begin{minipage}{\textwidth}
\begin{center}
\begin{tabular}{@{}lllll@{}}
\toprule
\multicolumn{1}{c}{Semantics of queried sample}   & Top & \multicolumn{1}{c}{Centerloss (sum)} & \multicolumn{1}{c}{Multi-label}     & \multicolumn{1}{c}{Single-label}             \\ \midrule
\multicolumn{1}{c}{agent, slocker, jisut, ransom} & 1   & agent, slocker, jisut, ransom  & agent, slocker, jisut, ransom & slocker, jisut, ransom                 \\
                                                  & 2   & agent, slocker, jisut, ransom  & agent, slocker, jisut, ransom & slocker, jisut, ransom                 \\
                                                  & 3   & agent, slocker, jisut, ransom  & agent, slocker, jisut, ransom & slocker, jisut, ransom                 \\
                                                  & 4   & agent, slocker, jisut, ransom  & agent, slocker, jisut, ransom & dropper, agent, slocker, jisut, ransom \\
                                                  & 5   & agent, slocker, jisut, ransom  & agent, slocker, jisut, ransom & slocker, jisut, ransom                 \\
                                                  & 6   & agent, slocker, jisut, ransom  & agent, slocker, jisut, ransom & dropper, agent, slocker, jisut, ransom \\
                                                  & 7   & agent, slocker, jisut, ransom  & slocker, jisut, ransom        & slocker                                \\
                                                  & 8   & agent, slocker, jisut, ransom  & agent, slocker, jisut, ransom & slocker, jisut, ransom                 \\
                                                  & 9   & agent, slocker, jisut, ransom  & slocker, jisut, ransom        & agent, slocker, jisut, ransom          \\
                                                  & 10  & agent, slocker, jisut, ransom  & slocker, jisut, ransom        & slocker, jisut, ransom                 \\ \bottomrule
\end{tabular}
\end{center}
\bigskip\centering
\end{minipage}
\end{table*}%

\textbf{Querying by semantic components. }
The next test is to querying the semantic components and retrieve samples. We tested the APK MR system learned by the centerloss (add) model. We used the sum of the semantic component vectors corresponding to \text{agent}, \text{ewind}, and \text{downloader} as a query.
\[
   query = \mathbf{s}_{\text{agent}} + \mathbf{s}_{\text{ewind}} + \mathbf{s}_{\text{downloader}}  
\]
Results can be found in Table.\ref{tab:semantics_query_result}. In our learning data set, there are 89 samples consisting of \text{agent}, \texttt{ewind}, and \texttt{downloader}. We could see that these samples were all searched in the samples up to the top 90.

% Table
\begin{table*}%semantics_query_result
\caption{Quering by semantics}
\label{tab:semantics_query_result}
\begin{minipage}{\textwidth}
\begin{center}
\begin{tabular}{@{}lcl@{}}
\toprule
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Queried semantic components\end{tabular}} & Top        & \multicolumn{1}{c}{Centerloss (sum)} \\ \midrule
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}agent, ewind,  downloader\end{tabular}}    & 1$\sim$86  & agent, ewind, downloader            \\
                                                                                           & 87         & agent, ewind, downloader, ransom    \\
                                                                                           & 88$\sim$90 & agent, ewind, downloader            \\
                                                                                           & 91         & agent, ewind, downloader, ransom    \\
                                                                                           & 92         & agent, ewind, downloader, ransom    \\
                                                                                           & 93         & downloader, agent, ewind, fakeinst  \\
                                                                                           & 94         & ewind, downloader                   \\ \bottomrule
\end{tabular}
\end{center}
\bigskip\centering
\end{minipage}
\end{table*}%


\textbf{Querying by the combination of semantics and sample. }
Finally, we tested to add or subtract some semantic components to the malware samples for querying. As in the previous two experiments, we tested for APK. The APK samples used in the experiment have four semantic components: \texttt{smsreg}, \texttt{agent}, \texttt{smspay}, \texttt{smssend}. Among these samples, the \texttt{smssend} component was omitted and the \texttt{dropper} component was added to create the query statement.
\[
   query = \mathbf{e_q} - \mathbf{s}_{\text{smssend}} + \mathbf{s}_{\text{dropper}} 
\]
In our learning data set, there are 56 samples consisting of four semantic components of \texttt{smsreg}, \texttt{agent}, \texttt{smspay}, and \texttt{dropper}, and all the samples are found in top 69 results.


% Table
\begin{table*}%sample_and_semantics_query_result
\caption{Querying by the combination of semantics and sample}
\label{tab:sample_and_semantics_query_result}
\begin{minipage}{\textwidth}
\begin{center}
\begin{tabular}{llcl}
\hline
\multicolumn{2}{c}{Query}                                                                    & \multirow{2}{*}{Top} & \multicolumn{1}{c}{\multirow{2}{*}{Centerloss (sum)}} \\
\multicolumn{1}{c}{Sample}                         & \multicolumn{1}{c}{Semantic components} &                      & \multicolumn{1}{c}{}                                 \\ \hline
\multicolumn{1}{c}{smsreg, agent, smspay, smssend} & - smssend + dropper                     & 1$\sim$34            & smsreg, dropper, agent, smspay                       \\
                                                   &                                         & 35                   & smsreg, dropper, smspay                              \\
                                                   &                                         & 36$\sim$45           & smsreg, dropper, agent, smspay                       \\
                                                   &                                         & 46                   & mobilepay, smsreg, dropper,agent, smspay             \\
                                                   &                                         & 47$\sim$49           & smsreg, dropper, agent, smspay                       \\
                                                   &                                         & 50                   & smsreg, dropper, agent, smspay, dowgin               \\
                                                   &                                         & 51                   & smsreg, dropper, agent, smspay                       \\ \hline
\end{tabular}
\end{center}
\bigskip\centering
\end{minipage}
\end{table*}%

%
%\subsection{Generalization}
%입력 Feature 가 Semantics-aware feature 일 수록 새로운 샘플에 대한 Error 가 더 작다. 우리가 제안한 목적함수는 멀티 레이블과 레이블 별 중요도로부터 해당 샘플이 시멘틱 스페이스에서 어디에 위치해야하는지를 가이드해준다. 따라서 Train Samples 에 대해서는 데이터의 입력 Feature가 Semantic-aware 하지 않더라도 원하는 곳에 특징 표현 벡터를 위치시킬 수 있다. 하지만 새로 보는 샘플의 representation vector 가 우리가 원하는 곳에 위치하게 된다는 보장은 없다. 딥러닝의 하이러키컬 특징 표현 학습이 MWC loss 를 통해 가이드 되는 Semantic을 학습할 수 있도록 하려면 입력 Feature가 충분히 Semantics-aware 해야 하고, 이에 관련된 연구들은 섹션2에서 소개하였다. 
%
%우리는 이 차이를 보이기 위해 semantics-aware level 이 다른 두 가지 피쳐에 대해 벡터 표현을 학습시키고, validation set 의 벡터 표현이 Semantic space 위에 잘 위치하는지 확인하는 실험을 한다. 첫 번째 실험에 사용되는 피쳐는 간단한 정적 분석을 통해 얻을 수 있는 Size, entropy\citep{} 이고, 두 번째 실험에 사용되는 피쳐는보다 조금 더 Semantics-aware 한 피쳐인 Thumbnail\citep{} 이다. 위에서 진행한 정량 평가와 정성 평가를 두 실험에 대해 진행해 본 결과 더 semantics-aware한 피쳐를 입력으로 사용할 수록 평가의 결과가 좋았고 이는 즉 Generalization 이 더 된다는 것을 의미한다. 즉, 누구라도 더 Semantics-aware한 피쳐를 입력으로 넣어줄 수 있다면, 간단히 Add-on 가능한 MWC loss 를 사용하여 훌륭한 Semantic space 를 구축할 수 있게 된다. 

% 시간이 된다면 APK Call-graph 도... 하면 좋을텐데..  

